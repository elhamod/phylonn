model:
  base_learning_rate: 4.5e-06
  project: Phylo-VQVAE-transformer
  target: taming.models.cond_transformer.Phylo_Net2NetTransformer
  params:
    cond_stage_key: class # key to condition (label in this case)
    phyloModel: True
    transformer_config:
      target: taming.modules.transformer.mingpt.GPT
      params:
        vocab_size: 64 # size of codebook (number of possible codes)
        block_size: 65 # # codes predicted at once.
        n_layer: 5
        n_head: 8 # relationships to learn
        n_embd: 16 #embedding dimensions for transformer. Does not have to be same as codebook but Ill pick same
    
    
    
    # from configs/custom_vqgan-256emb-256img-phylo-vqvae-phase4-8cbperlevel-lowerlr-stronganti.yaml
    first_stage_config:
      target: taming.models.phyloautoencoder.PhyloVQVAE
      project: Phylo-VQVAE
      params:
        # posttraining_ckpt: 
        ckpt_path: /fastscratch/elhamod/projects/taming-transformers/logs/256pixels_256embedding/checkpoints/last.ckpt
        embed_dim: 256
        n_embed: 1024
        ddconfig:
          double_z: False
          z_channels: 256
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult: [ 1,1,2,2,4]  # num_down = len(ch_mult)-1
          num_res_blocks: 2
          attn_resolutions: [16]
          dropout: 0.0
        lossconfig:
          target: taming.modules.losses.DummyLoss

        phylomodel_params:
          embed_dim: 16
          n_embed: 64
          #
          n_phylolevels: 4
          n_levels_non_attribute: 4 # equivalent of n_phylolevels for non_attributes
          codebooks_per_phylolevel: 8
          #
          n_phylo_channels: 64
          verbose: False
          #
          ch: 128
          # NOT hyperparameters.
          resolution: 16
          in_channels: 256
          out_ch: 256
          #ch_mult: [ 1]  # num_down = len(ch_mult)-1
          # num_res_blocks: 2
          # attn_resolutions: [16]
          n_mlp_layers: 1
          relu_last_layer: False #True
          repeatInput: False #True
          convin_switch: True #False

          lossconfig:
            target: taming.modules.losses.vqperceptual.VQLPIPSWithDiscriminator
            params:
              codebook_weight: 1.0
              disc_in_channels: 0
              disc_num_layers: 0
              disc_weight: 0.0
              disc_factor: 0.0
              perceptual_weight: 0.0
              disc_start: 10000
          
          lossconfig_phylo:
            target: taming.modules.losses.phyloloss.PhyloLoss
            params:
              phylo_weight: 0.1
              fc_layers: 1
              use_multiclass: True
              phyloDistances_string: "0.77,0.5,0.33"
              phylogenyconfig:
                target: taming.data.phylogeny.Phylogeny
                params:
                  filePath: /fastscratch/elhamod/data/Fish
          
          lossconfig_kernelorthogonality:
            target: taming.modules.losses.orthogonalloss.OrthogonalLoss
            params:
              weight: 1.0
              padding: 0
              stride: 1

          lossconfig_anticlassification:
            target: taming.modules.losses.anticlassificationloss.AntiClassificationLoss
            params:
              weight: 1.0
              beta: 0.7
              
          

      # target: taming.models.vqgan.VQModel
      # params:
      #   ckpt_path: logs/2020-11-09T13-33-36_faceshq_vqgan/checkpoints/last.ckpt
      #   embed_dim: 256
      #   n_embed: 1024
      #   ddconfig:
      #     double_z: false
      #     z_channels: 256
      #     resolution: 256
      #     in_channels: 3
      #     out_ch: 3
      #     ch: 128
      #     ch_mult:
      #     - 1
      #     - 1
      #     - 2
      #     - 2
      #     - 4
      #     num_res_blocks: 2
      #     attn_resolutions:
      #     - 16
      #     dropout: 0.0
        # lossconfig:
        #   target: taming.modules.losses.DummyLoss
    
    
    
    cond_stage_config:
      target: taming.modules.misc.label_conditioner.LabelCond
      params:
        num_of_classes: 38 #TODO: we might want to automate this.

data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 2
    num_workers: 8
    train:
      target: taming.data.custom.CustomTrain
      params:
        training_images_list_file: /home/elhamod/data/Fish/taming_transforms_fish_train_padded_256.txt
        size: 256
        add_labels: True
    validation:
      target: taming.data.custom.CustomTest
      params:
        test_images_list_file: /home/elhamod/data/Fish/taming_transforms_fish_test_padded_256.txt
        size: 256
        add_labels: True
